{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_14_7_Aug.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sw4nNhM8mN2z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9c3d3b4-2b5c-4e3b-90a6-e7dade92fc80"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import cv2\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VeLRe4cBmbu_",
        "outputId": "b3000bf9-634a-43a1-c370-64b3cfeca738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9Men1oDmesN",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q5EigjQNmhWU",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_random_crop(p=0.5,s_l=0.05,s_h=0.3,r_1=0.3,r_2=1/0.3,max_erasers_per_image=1,pixel_level=True,random_crop_size=(32,32),padding_pixels=4):\n",
        "  \n",
        "  assert max_erasers_per_image>=1 \n",
        "  def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        mx = np.random.randint(1,max_erasers_per_image+1)\n",
        "        for i in range(mx):\n",
        "          while True:\n",
        "              s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "              r = np.random.uniform(r_1, r_2)\n",
        "              w = int(np.sqrt(s / r))\n",
        "              h = int(np.sqrt(s * r))\n",
        "              left = np.random.randint(0, img_w)\n",
        "              top = np.random.randint(0, img_h)\n",
        "\n",
        "              if left + w <= img_w and top + h <= img_h:\n",
        "                  break\n",
        "\n",
        "          if pixel_level:\n",
        "              c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "          else:\n",
        "              c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "          input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "\n",
        "    \n",
        "  def random_crop(input_image):\n",
        "    assert input_image.shape[2]==3\n",
        "\n",
        "    #pad for 4 pixels\n",
        "    img = cv2.copyMakeBorder(input_image,padding_pixels,padding_pixels,padding_pixels,padding_pixels,cv2.BORDER_REPLICATE)\n",
        "    height , width =img.shape[0],img.shape[1]\n",
        "    dy,dx = random_crop_size\n",
        "    x = np.random.randint(0,width - dx + 1)\n",
        "    y = np.random.randint(0,height - dy + 1)\n",
        "    return img[y:(y+dy),x:(x+dx),:]\n",
        "\n",
        "  def preprocess_image(input_image):\n",
        "    return eraser(random_crop(input_image))\n",
        "  \n",
        "  return preprocess_image\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tOuprdCpqSB",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=0.5,featurewise_center=True, featurewise_std_normalization=True,preprocessing_function=get_cutout_eraser_and_random_crop())\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "#datagen.fit(train_features)\n",
        "train_generator = datagen.flow(train_features,train_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMi1CeDsqUyv",
        "colab": {}
      },
      "source": [
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "test_datagen.fit(test_features)\n",
        "test_generator = test_datagen.flow(test_features,test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Za7Xax0zyZc2"
      },
      "source": [
        "#Resnet Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xp2uGvCQcTq",
        "colab_type": "code",
        "outputId": "4c7e483a-5490-4802-f8f8-b9b9b728ec9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# import time, math\n",
        "# def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "#   fan = np.prod(shape[:-1])\n",
        "#   bound = 1 / math.sqrt(fan)\n",
        "#   return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "initializer = tf.keras.initializers.glorot_normal(seed=None)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 09:57:21.262352 140373475358592 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B41YNxNtCUwa",
        "colab": {}
      },
      "source": [
        "def ResNetBlock(input_layer, channels,stride=1):\n",
        "  \n",
        "  bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(input_layer)\n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  if(stride==2):\n",
        "    print(\"here\")\n",
        "    block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3),dilation_rate=(1,1) ,padding='same',kernel_initializer=initializer,use_bias=False)(activation_layer_b1)\n",
        "    block_layer_1= tf.keras.layers.MaxPooling2D()(block_layer_1)\n",
        "  else:\n",
        "    block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',dilation_rate=(1,1),kernel_initializer=initializer,use_bias=False)(activation_layer_b1)\n",
        "  \n",
        "  bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "  activation_layer_b2 = tf.keras.layers.Activation('relu')(bn_2) \n",
        "  block_layer_2 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',dilation_rate=(1,1),kernel_initializer=initializer,use_bias=False)(activation_layer_b2)\n",
        "   \n",
        "  \n",
        "  return block_layer_2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zl45v4UBTff",
        "outputId": "2c5248c5-75f7-41e2-c6fe-c5c26619eac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# from tf.keras.layers import Input, add, GlobalAveragePooling2D, Dense\n",
        "#from tf.keras.models import Model\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "x1 = tf.keras.layers.Conv2D(32 ,(3, 3),padding='same',kernel_initializer=initializer,use_bias=False)(inputs)   #32x32 \n",
        "activation_x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "bn1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x1)\n",
        "\n",
        "# x2 = tf.keras.layers.Conv2D(32, (3, 3),padding='same',kernel_initializer=initializer)(bn1)   #32x32 \n",
        "# activation_x2 = tf.keras.layers.Activation('relu')(x2)\n",
        "# bn2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x2)\n",
        "\n",
        "# x3 = tf.keras.layers.Conv2D(32, (3, 3),padding='same')(activation_x2)   #32x32 \n",
        "# activation_x3 = tf.keras.layers.Activation('relu')(x3)\n",
        "# bn3 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x3)\n",
        "\n",
        "##block 1\n",
        "\n",
        "\n",
        "\n",
        "blk1 = ResNetBlock(bn1,32)  ##32x32\n",
        "z1 = tf.keras.layers.add([blk1,bn1])\n",
        "\n",
        "blk1_c = ResNetBlock(z1,32)\n",
        "z1_c = tf.keras.layers.add([blk1_c,z1])\n",
        "\n",
        "\n",
        "\n",
        "##block 2\n",
        "\n",
        "blk2 = ResNetBlock(z1_c,64,stride=2)\n",
        "mx_1= tf.keras.layers.MaxPooling2D()(z1_c)\n",
        "one_blk = tf.keras.layers.Conv2D(64, (1, 1), padding='same',kernel_initializer=initializer,use_bias=False)(mx_1)\n",
        "z2 = tf.keras.layers.add([blk2,one_blk])\n",
        "\n",
        "blk2_c = ResNetBlock(z2,64)\n",
        "z2_c = tf.keras.layers.add([blk2_c,z2])\n",
        "\n",
        "\n",
        "\n",
        "##block3\n",
        "\n",
        "blk3 = ResNetBlock(z2_c,128,stride=2)\n",
        "mx_2= tf.keras.layers.MaxPooling2D()(z2_c)\n",
        "\n",
        "one_blk_1 = tf.keras.layers.Conv2D(128, (1, 1), padding='same',kernel_initializer=initializer,use_bias=False)(mx_2)\n",
        "z3 = tf.keras.layers.add([blk3,one_blk_1])\n",
        "\n",
        "blk3_c = ResNetBlock(z3,128)\n",
        "z3_c = tf.keras.layers.add([blk3_c,z3])\n",
        "\n",
        "\n",
        "##block4\n",
        "\n",
        "blk4 = ResNetBlock(z3_c,256,stride=2)\n",
        "mx_3= tf.keras.layers.MaxPooling2D()(z3_c)\n",
        "\n",
        "one_blk_2 = tf.keras.layers.Conv2D(256, (1, 1), padding='same')(mx_3)\n",
        "z4 = tf.keras.layers.add([blk4,one_blk_2])\n",
        "\n",
        "blk4_c = ResNetBlock(z4,256)\n",
        "z4_c = tf.keras.layers.add([blk4_c,z4])\n",
        "\n",
        "avg_pool_layer = tf.keras.layers.GlobalAveragePooling2D()(z4_c)\n",
        "\n",
        "#flatten_layer = Flatten()(avg_pool_layer)\n",
        "\n",
        "fc_layer = tf.keras.layers.Dense(10, activation='softmax')(avg_pool_layer)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= fc_layer)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here\n",
            "here\n",
            "here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sV-nVdlMDwva",
        "outputId": "547f933e-f10b-4fb7-cf97-6f0443536d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9216        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9216        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9216        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   18432       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 64)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   36864       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   2048        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    147456      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 128)    8192        max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 128)    0           conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147456      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 128)    0           conv2d_14[0][0]                  \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 256)    294912      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 256)    0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    589824      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 256)    0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    589824      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    589824      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 256)    0           conv2d_19[0][0]                  \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 256)          0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 2,799,978\n",
            "Trainable params: 2,796,522\n",
            "Non-trainable params: 3,456\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9oBxPClHadk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot_model(model, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZl_uafGb3r9",
        "colab": {}
      },
      "source": [
        "# #from one_cycle_lr import LRFinder\n",
        "# from one_cycle_lr_tf import LRFinder\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDMPrA1Gj01s"
      },
      "source": [
        "#Best LR would be 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPvfJUU1sn1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def truncate(n, decimals=0):\n",
        "#     multiplier = 10 ** decimals\n",
        "#     return int(n * multiplier) / multiplier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5l4Uuj2MLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## for 24 epochs only\n",
        "\n",
        "MAX_LR= 0.05\n",
        "base_lr = 0.01\n",
        "\n",
        "def lr_func(epoch,lr):\n",
        "  lr = base_lr\n",
        "  max_lr = MAX_LR\n",
        "  \n",
        "  if(epoch == 0):\n",
        "    lr = base_lr\n",
        "  elif(epoch>0 and epoch<11):\n",
        "    lr += (max_lr-base_lr)*(epoch)/11\n",
        "  else:\n",
        "    lr = max_lr - (max_lr-base_lr)*(epoch-11)/18\n",
        "  print(\"final lr \",round(lr,5))\n",
        "  return round(lr,5)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjzOgXaWj_uE",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(momentum=0.9,nesterov=True)\n",
        "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XaDmx-zQlExS",
        "colab": {}
      },
      "source": [
        "# num_samples= train_features.shape[0]\n",
        "# batch_size =128\n",
        "# num_epoch=24\n",
        "# max_lr=0.1\n",
        "\n",
        "# from one_cycle_lr_tf import OneCycleLR\n",
        "\n",
        "# lr_manager = OneCycleLR(num_samples, num_epoch, batch_size, max_lr,\n",
        "#                         end_percentage=0.1, scale_percentage=None,\n",
        "#                         maximum_momentum=0.95, minimum_momentum=0.85)\n",
        "\n",
        "# opt = tf.keras.optimizers.SGD()\n",
        "# model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "                        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "upsNiPO4JWjb",
        "outputId": "b8915c0d-557c-4ba3-af10-eefb7772c11f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath = \"Resnet-13-test1.hdf5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "##Train the model\n",
        "model_info = model.fit_generator(train_generator,\n",
        "                                 steps_per_epoch=np.ceil(50000/128), epochs=30,  \n",
        "                                 validation_data = test_generator, verbose=1,callbacks=[checkpoint,LearningRateScheduler(lr_func, verbose=1)])\n",
        "\n",
        "\n",
        "# model_info = model.fit_generator(train_generator,\n",
        "#                                  steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "#                                  validation_data = test_generator, verbose=1,callbacks=[checkpoint,lr_manager])\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final lr  0.01\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.4346 - acc: 0.4958\n",
            "Epoch 00001: val_acc improved from -inf to 0.58420, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 59s 150ms/step - loss: 1.4341 - acc: 0.4959 - val_loss: 1.1749 - val_acc: 0.5842\n",
            "final lr  0.01364\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01364.\n",
            "Epoch 2/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0018 - acc: 0.6430\n",
            "Epoch 00002: val_acc improved from 0.58420 to 0.62990, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 1.0015 - acc: 0.6431 - val_loss: 1.0829 - val_acc: 0.6299\n",
            "final lr  0.01727\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01727.\n",
            "Epoch 3/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.7090\n",
            "Epoch 00003: val_acc improved from 0.62990 to 0.70890, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.8307 - acc: 0.7092 - val_loss: 0.8471 - val_acc: 0.7089\n",
            "final lr  0.02091\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.02091.\n",
            "Epoch 4/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7376 - acc: 0.7419\n",
            "Epoch 00004: val_acc improved from 0.70890 to 0.76770, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 129ms/step - loss: 0.7373 - acc: 0.7420 - val_loss: 0.6754 - val_acc: 0.7677\n",
            "final lr  0.02455\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.02455.\n",
            "Epoch 5/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6647 - acc: 0.7694\n",
            "Epoch 00005: val_acc improved from 0.76770 to 0.78140, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.6645 - acc: 0.7694 - val_loss: 0.6620 - val_acc: 0.7814\n",
            "final lr  0.02818\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.02818.\n",
            "Epoch 6/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6210 - acc: 0.7831\n",
            "Epoch 00006: val_acc improved from 0.78140 to 0.79740, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.6209 - acc: 0.7831 - val_loss: 0.6207 - val_acc: 0.7974\n",
            "final lr  0.03182\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.03182.\n",
            "Epoch 7/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5863 - acc: 0.7979\n",
            "Epoch 00007: val_acc improved from 0.79740 to 0.83230, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 129ms/step - loss: 0.5861 - acc: 0.7980 - val_loss: 0.5187 - val_acc: 0.8323\n",
            "final lr  0.03545\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.03545.\n",
            "Epoch 8/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5556 - acc: 0.8085\n",
            "Epoch 00008: val_acc did not improve from 0.83230\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.5560 - acc: 0.8084 - val_loss: 0.5106 - val_acc: 0.8311\n",
            "final lr  0.03909\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.03909.\n",
            "Epoch 9/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5214 - acc: 0.8191\n",
            "Epoch 00009: val_acc did not improve from 0.83230\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.5213 - acc: 0.8191 - val_loss: 0.5190 - val_acc: 0.8320\n",
            "final lr  0.04273\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.04273.\n",
            "Epoch 10/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5035 - acc: 0.8261\n",
            "Epoch 00010: val_acc did not improve from 0.83230\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.5034 - acc: 0.8263 - val_loss: 0.5690 - val_acc: 0.8198\n",
            "final lr  0.04636\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.04636.\n",
            "Epoch 11/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4761 - acc: 0.8356\n",
            "Epoch 00011: val_acc did not improve from 0.83230\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.4758 - acc: 0.8357 - val_loss: 0.6171 - val_acc: 0.8111\n",
            "final lr  0.05\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.05.\n",
            "Epoch 12/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4734 - acc: 0.8375\n",
            "Epoch 00012: val_acc improved from 0.83230 to 0.85440, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.4734 - acc: 0.8376 - val_loss: 0.4529 - val_acc: 0.8544\n",
            "final lr  0.04778\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.04778.\n",
            "Epoch 13/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4294 - acc: 0.8511\n",
            "Epoch 00013: val_acc improved from 0.85440 to 0.85880, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.4290 - acc: 0.8512 - val_loss: 0.4356 - val_acc: 0.8588\n",
            "final lr  0.04556\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.04556.\n",
            "Epoch 14/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8646\n",
            "Epoch 00014: val_acc improved from 0.85880 to 0.86730, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.3896 - acc: 0.8645 - val_loss: 0.4099 - val_acc: 0.8673\n",
            "final lr  0.04333\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.04333.\n",
            "Epoch 15/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.8731\n",
            "Epoch 00015: val_acc improved from 0.86730 to 0.86900, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.3677 - acc: 0.8730 - val_loss: 0.3989 - val_acc: 0.8690\n",
            "final lr  0.04111\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.04111.\n",
            "Epoch 16/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3431 - acc: 0.8806\n",
            "Epoch 00016: val_acc improved from 0.86900 to 0.88170, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.3429 - acc: 0.8806 - val_loss: 0.3637 - val_acc: 0.8817\n",
            "final lr  0.03889\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.03889.\n",
            "Epoch 17/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8875\n",
            "Epoch 00017: val_acc improved from 0.88170 to 0.88300, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.3246 - acc: 0.8875 - val_loss: 0.3714 - val_acc: 0.8830\n",
            "final lr  0.03667\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.03667.\n",
            "Epoch 18/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2978 - acc: 0.8967\n",
            "Epoch 00018: val_acc did not improve from 0.88300\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.2978 - acc: 0.8967 - val_loss: 0.3921 - val_acc: 0.8788\n",
            "final lr  0.03444\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.03444.\n",
            "Epoch 19/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2790 - acc: 0.9025\n",
            "Epoch 00019: val_acc improved from 0.88300 to 0.89970, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.2789 - acc: 0.9025 - val_loss: 0.3376 - val_acc: 0.8997\n",
            "final lr  0.03222\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.03222.\n",
            "Epoch 20/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.9080\n",
            "Epoch 00020: val_acc did not improve from 0.89970\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.2582 - acc: 0.9080 - val_loss: 0.3427 - val_acc: 0.8921\n",
            "final lr  0.03\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.03.\n",
            "Epoch 21/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2487 - acc: 0.9112\n",
            "Epoch 00021: val_acc did not improve from 0.89970\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.2486 - acc: 0.9112 - val_loss: 0.3317 - val_acc: 0.8957\n",
            "final lr  0.02778\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.02778.\n",
            "Epoch 22/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9178\n",
            "Epoch 00022: val_acc improved from 0.89970 to 0.90180, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.2341 - acc: 0.9178 - val_loss: 0.3243 - val_acc: 0.9018\n",
            "final lr  0.02556\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.02556.\n",
            "Epoch 23/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9244\n",
            "Epoch 00023: val_acc did not improve from 0.90180\n",
            "391/391 [==============================] - 50s 127ms/step - loss: 0.2145 - acc: 0.9245 - val_loss: 0.3243 - val_acc: 0.8998\n",
            "final lr  0.02333\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.02333.\n",
            "Epoch 24/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2015 - acc: 0.9289\n",
            "Epoch 00024: val_acc did not improve from 0.90180\n",
            "391/391 [==============================] - 50s 127ms/step - loss: 0.2016 - acc: 0.9289 - val_loss: 0.3281 - val_acc: 0.9014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaHWm9EDWQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "679e0d29-9da7-4892-f0fa-03232bbff6e7"
      },
      "source": [
        "##Train the model\n",
        "model_info = model.fit_generator(train_generator,\n",
        "                                 steps_per_epoch=np.ceil(50000/128), epochs=6,  \n",
        "                                 validation_data = test_generator, verbose=1,callbacks=[checkpoint,LearningRateScheduler(lr_func, verbose=1)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final lr  0.01\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/6\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.9402\n",
            "Epoch 00001: val_acc improved from 0.90180 to 0.91580, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 50s 129ms/step - loss: 0.1672 - acc: 0.9403 - val_loss: 0.2788 - val_acc: 0.9158\n",
            "final lr  0.01364\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01364.\n",
            "Epoch 2/6\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9422\n",
            "Epoch 00002: val_acc did not improve from 0.91580\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.1634 - acc: 0.9421 - val_loss: 0.3012 - val_acc: 0.9100\n",
            "final lr  0.01727\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01727.\n",
            "Epoch 3/6\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9412\n",
            "Epoch 00003: val_acc did not improve from 0.91580\n",
            "391/391 [==============================] - 50s 128ms/step - loss: 0.1666 - acc: 0.9411 - val_loss: 0.3087 - val_acc: 0.9096\n",
            "final lr  0.02091\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.02091.\n",
            "Epoch 4/6\n",
            "337/391 [========================>.....] - ETA: 6s - loss: 0.1804 - acc: 0.9369"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ADKRdQnkKQ_z",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGlxPxyywoIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lMNqRYEAzTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}