{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_13_v-tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sw4nNhM8mN2z",
        "outputId": "08962efc-16f0-4726-83cf-6de88b86e487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import cv2\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VeLRe4cBmbu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "90156f9d-739a-4612-ea3b-07abaf073ee7"
      },
      "source": [
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9Men1oDmesN",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q5EigjQNmhWU",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_random_crop(p=0.5,s_l=0.05,s_h=0.3,r_1=0.3,r_2=1/0.3,max_erasers_per_image=1,pixel_level=True,random_crop_size=(32,32),padding_pixels=4):\n",
        "  \n",
        "  assert max_erasers_per_image>=1 \n",
        "  def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        mx = np.random.randint(1,max_erasers_per_image+1)\n",
        "        for i in range(mx):\n",
        "          while True:\n",
        "              s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "              r = np.random.uniform(r_1, r_2)\n",
        "              w = int(np.sqrt(s / r))\n",
        "              h = int(np.sqrt(s * r))\n",
        "              left = np.random.randint(0, img_w)\n",
        "              top = np.random.randint(0, img_h)\n",
        "\n",
        "              if left + w <= img_w and top + h <= img_h:\n",
        "                  break\n",
        "\n",
        "          if pixel_level:\n",
        "              c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "          else:\n",
        "              c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "          input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "\n",
        "    \n",
        "  def random_crop(input_image):\n",
        "    assert input_image.shape[2]==3\n",
        "\n",
        "    #pad for 4 pixels\n",
        "    img = cv2.copyMakeBorder(input_image,padding_pixels,padding_pixels,padding_pixels,padding_pixels,cv2.BORDER_REPLICATE)\n",
        "    height , width =img.shape[0],img.shape[1]\n",
        "    dy,dx = random_crop_size\n",
        "    x = np.random.randint(0,width - dx + 1)\n",
        "    y = np.random.randint(0,height - dy + 1)\n",
        "    return img[y:(y+dy),x:(x+dx),:]\n",
        "\n",
        "  def preprocess_image(input_image):\n",
        "    return eraser(random_crop(input_image))\n",
        "  \n",
        "  return preprocess_image\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tOuprdCpqSB",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=0.5,featurewise_center=True, featurewise_std_normalization=True,preprocessing_function=get_cutout_eraser_and_random_crop())\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "#datagen.fit(train_features)\n",
        "train_generator = datagen.flow(train_features,train_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMi1CeDsqUyv",
        "colab": {}
      },
      "source": [
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "test_datagen.fit(test_features)\n",
        "test_generator = test_datagen.flow(test_features,test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Za7Xax0zyZc2"
      },
      "source": [
        "#Resnet Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B41YNxNtCUwa",
        "colab": {}
      },
      "source": [
        "def ResNetBlock(input_layer, channels,stride=1):\n",
        "  \n",
        "  bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(input_layer)\n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',strides=stride)(activation_layer_b1)\n",
        "  \n",
        "  bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "  activation_layer_b2 = tf.keras.layers.Activation('relu')(bn_2) \n",
        "  block_layer_2 = tf.keras.layers.Conv2D(channels, (3,3), padding='same')(activation_layer_b2)\n",
        "   \n",
        "  \n",
        "  return block_layer_2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zl45v4UBTff",
        "outputId": "0aaccae7-8ad4-40c1-effc-3feffcef908a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# from tf.keras.layers import Input, add, GlobalAveragePooling2D, Dense\n",
        "#from tf.keras.models import Model\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "x1 = tf.keras.layers.Conv2D(32, (3, 3),padding='same')(inputs)   #32x32 \n",
        "activation_x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "bn1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x1)\n",
        "\n",
        "# x2 = tf.keras.layers.Conv2D(64, (3, 3),padding='same')(activation_x1)   #32x32 \n",
        "# activation_x2 = tf.keras.layers.Activation('relu')(x2)\n",
        "\n",
        "# x3 = tf.keras.layers.Conv2D(64, (3, 3),padding='same')(activation_x2)   #32x32 \n",
        "# activation_x3 = tf.keras.layers.Activation('relu')(x3)\n",
        "\n",
        "\n",
        "##block 1\n",
        "\n",
        "blk1 = ResNetBlock(bn1,32)  ##32x32\n",
        "z1 = tf.keras.layers.add([blk1,bn1])\n",
        "\n",
        "blk1_c = ResNetBlock(z1,32)\n",
        "z1_c = tf.keras.layers.add([blk1_c,z1])\n",
        "\n",
        "##block 2\n",
        "\n",
        "blk2 = ResNetBlock(z1_c,64)\n",
        "one_blk = tf.keras.layers.Conv2D(64, (1, 1), padding='same')(z1_c)\n",
        "z2 = tf.keras.layers.add([blk2,one_blk])\n",
        "\n",
        "blk2_c = ResNetBlock(z2,64)\n",
        "z2_c = tf.keras.layers.add([blk2_c,z2])\n",
        "\n",
        "##block3\n",
        "\n",
        "blk3 = ResNetBlock(z2_c,128,stride=2)\n",
        "one_blk_1 = tf.keras.layers.Conv2D(128, (1, 1), padding='same',strides=2)(z2_c)\n",
        "z3 = tf.keras.layers.add([blk3,one_blk_1])\n",
        "\n",
        "blk3_c = ResNetBlock(z3,128)\n",
        "z3_c = tf.keras.layers.add([blk3_c,z3])\n",
        "\n",
        "##block4\n",
        "\n",
        "blk4 = ResNetBlock(z3_c,256,stride=2)\n",
        "one_blk_2 = tf.keras.layers.Conv2D(256, (1, 1), padding='same',strides=2)(z3_c)\n",
        "z4 = tf.keras.layers.add([blk4,one_blk_2])\n",
        "\n",
        "blk4_c = ResNetBlock(z4,256)\n",
        "z4_c = tf.keras.layers.add([blk4_c,z4])\n",
        "\n",
        "avg_pool_layer = tf.keras.layers.GlobalAveragePooling2D()(z4_c)\n",
        "\n",
        "#flatten_layer = Flatten()(avg_pool_layer)\n",
        "\n",
        "fc_layer = tf.keras.layers.Dense(10, activation='softmax')(avg_pool_layer)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= fc_layer)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0805 14:09:37.377666 140136801228672 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sV-nVdlMDwva",
        "outputId": "79fb5743-56b4-47b3-c709-9e23fce77024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   896         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9248        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   18496       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36928       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   2112        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 64)   36928       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 64)   36928       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  73856       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  8320        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  147584      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  147584      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           conv2d_14[0][0]                  \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 256)    295168      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 8, 8, 256)    1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 8, 8, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 256)    590080      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 8, 8, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 256)    590080      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 8, 8, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 256)    590080      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_19[0][0]                  \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 256)          0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 2,802,122\n",
            "Trainable params: 2,798,666\n",
            "Non-trainable params: 3,456\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZl_uafGb3r9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "417010e5-d33b-4949-f3a9-b12d5a955b88"
      },
      "source": [
        "#from one_cycle_lr import LRFinder\n",
        "from one_cycle_lr_tf import LRFinder\n",
        "num_samples= train_features.shape[0]\n",
        "batch_size =512\n",
        "num_epoch=50\n",
        "max_lr=0.05\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9442b408a4e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mone_cycle_lr_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'one_cycle_lr_tf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDMPrA1Gj01s"
      },
      "source": [
        "#Best LR would be 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yioj4DyCwzwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "EPOCHS=24\n",
        "LEARNING_RATE=0.01\n",
        "len_train=50000\n",
        "BATCH_SIZE=512\n",
        "MOMENTUM=0.9\n",
        "\n",
        "batches_per_epoch = len_train//BATCH_SIZE + 1\n",
        "\n",
        "lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])[0]\n",
        "global_step = tf.train.get_or_create_global_step()\n",
        "lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
        "opt = tf.train.MomentumOptimizer(lr_func, momentum=MOMENTUM, use_nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5l4Uuj2MLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## for 24 epochs only\n",
        "\n",
        "MAX_LR= 0.1\n",
        "base_lr = 0.01\n",
        "\n",
        "def lr_func(epoch,lr):\n",
        "  lr = base_lr\n",
        "  max_lr = MAX_LR\n",
        "  one_step = (max_lr-base_lr)/16\n",
        "  if(epoch == 0):\n",
        "    return lr\n",
        "  elif(epoch>0 and epoch<8):\n",
        "    lr += (max_lr-base_lr)*(epoch)/7\n",
        "  else:\n",
        "    lr = max_lr - (max_lr-base_lr)*(epoch-7)/16 \n",
        "  #base_lr = lr\n",
        "  return lr\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjzOgXaWj_uE",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XaDmx-zQlExS",
        "colab": {}
      },
      "source": [
        "\n",
        "# from one_cycle_lr_tf import OneCycleLR\n",
        "\n",
        "# lr_manager = OneCycleLR(num_samples, num_epoch, batch_size, max_lr,\n",
        "#                         end_percentage=0.1, scale_percentage=None,\n",
        "#                         maximum_momentum=0.95, minimum_momentum=0.85)\n",
        "                        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sDeQk2bZqTGp",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7rCtZuj2AXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "upsNiPO4JWjb",
        "outputId": "4ad8597b-4505-49d8-9573-bcd11b0910ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath = \"Resnet-13-test1.hdf5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model_info = model.fit_generator(train_generator,\n",
        "                                 steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "                                 validation_data = test_generator, verbose=1,callbacks=[LearningRateScheduler(lr_func, verbose=1)])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/24\n",
            "391/391 [==============================] - 57s 145ms/step - loss: 1.6641 - acc: 0.4080 - val_loss: 1.5185 - val_acc: 0.4798\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.02285714285714286.\n",
            "Epoch 2/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 1.2785 - acc: 0.5388 - val_loss: 1.0842 - val_acc: 0.6073\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.03571428571428572.\n",
            "Epoch 3/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 1.1076 - acc: 0.6077 - val_loss: 1.1299 - val_acc: 0.6006\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.04857142857142858.\n",
            "Epoch 4/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 1.0089 - acc: 0.6496 - val_loss: 1.1058 - val_acc: 0.6535\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.06142857142857144.\n",
            "Epoch 5/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.9118 - acc: 0.6885 - val_loss: 0.7854 - val_acc: 0.7368\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.07428571428571429.\n",
            "Epoch 6/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.8124 - acc: 0.7208 - val_loss: 0.7319 - val_acc: 0.7559\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.08714285714285715.\n",
            "Epoch 7/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 0.7318 - acc: 0.7517 - val_loss: 0.7409 - val_acc: 0.7530\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
            "Epoch 8/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.6887 - acc: 0.7665 - val_loss: 0.7641 - val_acc: 0.7501\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.094375.\n",
            "Epoch 9/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.5848 - acc: 0.8001 - val_loss: 0.5482 - val_acc: 0.8165\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.08875000000000001.\n",
            "Epoch 10/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.5145 - acc: 0.8225 - val_loss: 0.5340 - val_acc: 0.8184\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.083125.\n",
            "Epoch 11/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.4748 - acc: 0.8384 - val_loss: 0.4913 - val_acc: 0.8367\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0775.\n",
            "Epoch 12/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.4342 - acc: 0.8511 - val_loss: 0.5339 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.071875.\n",
            "Epoch 13/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.3990 - acc: 0.8622 - val_loss: 0.4904 - val_acc: 0.8515\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.06625.\n",
            "Epoch 14/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.3679 - acc: 0.8716 - val_loss: 0.3733 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.060625.\n",
            "Epoch 15/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.3391 - acc: 0.8825 - val_loss: 0.3710 - val_acc: 0.8789\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.055.\n",
            "Epoch 16/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 0.3124 - acc: 0.8910 - val_loss: 0.3542 - val_acc: 0.8836\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.049375.\n",
            "Epoch 17/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 0.2937 - acc: 0.8978 - val_loss: 0.3367 - val_acc: 0.8920\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.04375.\n",
            "Epoch 18/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 0.2597 - acc: 0.9083 - val_loss: 0.3543 - val_acc: 0.8863\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.038125.\n",
            "Epoch 19/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 0.2430 - acc: 0.9148 - val_loss: 0.3167 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0325.\n",
            "Epoch 20/24\n",
            "391/391 [==============================] - 48s 124ms/step - loss: 0.2217 - acc: 0.9230 - val_loss: 0.2916 - val_acc: 0.9058\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.026874999999999996.\n",
            "Epoch 21/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.2011 - acc: 0.9302 - val_loss: 0.2835 - val_acc: 0.9119\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.02124999999999999.\n",
            "Epoch 22/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.1875 - acc: 0.9343 - val_loss: 0.2847 - val_acc: 0.9110\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.015625.\n",
            "Epoch 23/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.1633 - acc: 0.9420 - val_loss: 0.2596 - val_acc: 0.9196\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.009999999999999995.\n",
            "Epoch 24/24\n",
            "391/391 [==============================] - 48s 123ms/step - loss: 0.1511 - acc: 0.9464 - val_loss: 0.2650 - val_acc: 0.9179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ADKRdQnkKQ_z",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGlxPxyywoIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_func"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}