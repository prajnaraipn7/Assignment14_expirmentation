{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_13_custom.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sw4nNhM8mN2z",
        "outputId": "988eefb3-5f1c-4d5c-9432-190d64a5390d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import cv2\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VeLRe4cBmbu_",
        "outputId": "f51ba00d-80c6-49b4-d279-8773e984802d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 14s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9Men1oDmesN",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q5EigjQNmhWU",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_random_crop(p=0.5,s_l=0.05,s_h=0.3,r_1=0.3,r_2=1/0.3,max_erasers_per_image=1,pixel_level=True,random_crop_size=(32,32),padding_pixels=4):\n",
        "  \n",
        "  assert max_erasers_per_image>=1 \n",
        "  def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        mx = np.random.randint(1,max_erasers_per_image+1)\n",
        "        for i in range(mx):\n",
        "          while True:\n",
        "              s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "              r = np.random.uniform(r_1, r_2)\n",
        "              w = int(np.sqrt(s / r))\n",
        "              h = int(np.sqrt(s * r))\n",
        "              left = np.random.randint(0, img_w)\n",
        "              top = np.random.randint(0, img_h)\n",
        "\n",
        "              if left + w <= img_w and top + h <= img_h:\n",
        "                  break\n",
        "\n",
        "          if pixel_level:\n",
        "              c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "          else:\n",
        "              c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "          input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "\n",
        "    \n",
        "  def random_crop(input_image):\n",
        "    assert input_image.shape[2]==3\n",
        "\n",
        "    #pad for 4 pixels\n",
        "    img = cv2.copyMakeBorder(input_image,padding_pixels,padding_pixels,padding_pixels,padding_pixels,cv2.BORDER_REPLICATE)\n",
        "    height , width =img.shape[0],img.shape[1]\n",
        "    dy,dx = random_crop_size\n",
        "    x = np.random.randint(0,width - dx + 1)\n",
        "    y = np.random.randint(0,height - dy + 1)\n",
        "    return img[y:(y+dy),x:(x+dx),:]\n",
        "\n",
        "  def preprocess_image(input_image):\n",
        "    return eraser(random_crop(input_image))\n",
        "  \n",
        "  return preprocess_image\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tOuprdCpqSB",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=0.5,featurewise_center=True, featurewise_std_normalization=True,preprocessing_function=get_cutout_eraser_and_random_crop())\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "#datagen.fit(train_features)\n",
        "train_generator = datagen.flow(train_features,train_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMi1CeDsqUyv",
        "colab": {}
      },
      "source": [
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "test_datagen.fit(test_features)\n",
        "test_generator = test_datagen.flow(test_features,test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Za7Xax0zyZc2"
      },
      "source": [
        "#Resnet Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xp2uGvCQcTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ce538093-a3a8-4bc9-c540-e1b0ac3ec31d"
      },
      "source": [
        "# import time, math\n",
        "# def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "#   fan = np.prod(shape[:-1])\n",
        "#   bound = 1 / math.sqrt(fan)\n",
        "#   return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "initializer = tf.keras.initializers.glorot_normal(seed=None)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0806 15:00:59.138075 139948562438016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B41YNxNtCUwa",
        "colab": {}
      },
      "source": [
        "def ResNetBlock(input_layer, channels,stride=1):\n",
        "  \n",
        "  bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(input_layer)\n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  if(stride==2):\n",
        "    activation_layer_b1= tf.keras.layers.MaxPooling2D()(activation_layer_b1)\n",
        "  block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=initializer,use_bias=False)(activation_layer_b1)\n",
        "  \n",
        "  bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "  activation_layer_b2 = tf.keras.layers.Activation('relu')(bn_2) \n",
        "  block_layer_2 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=initializer,use_bias=False)(activation_layer_b2)\n",
        "   \n",
        "  \n",
        "  return block_layer_2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zl45v4UBTff",
        "colab": {}
      },
      "source": [
        "# from tf.keras.layers import Input, add, GlobalAveragePooling2D, Dense\n",
        "#from tf.keras.models import Model\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "x1 = tf.keras.layers.Conv2D(32, (3, 3),padding='same',kernel_initializer=initializer,use_bias=False)(inputs)   #32x32 \n",
        "activation_x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "bn1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x1)\n",
        "\n",
        "# x2 = tf.keras.layers.Conv2D(32, (3, 3),padding='same',kernel_initializer=initializer)(bn1)   #32x32 \n",
        "# activation_x2 = tf.keras.layers.Activation('relu')(x2)\n",
        "# bn2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x2)\n",
        "\n",
        "# x3 = tf.keras.layers.Conv2D(32, (3, 3),padding='same')(activation_x2)   #32x32 \n",
        "# activation_x3 = tf.keras.layers.Activation('relu')(x3)\n",
        "# bn3 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x3)\n",
        "\n",
        "##block 1\n",
        "\n",
        "blk1 = ResNetBlock(bn1,32)  ##32x32\n",
        "z1 = tf.keras.layers.add([blk1,bn1])\n",
        "\n",
        "blk1_c = ResNetBlock(z1,32)\n",
        "z1_c = tf.keras.layers.add([blk1_c,z1])\n",
        "\n",
        "\n",
        "\n",
        "##block 2\n",
        "\n",
        "blk2 = ResNetBlock(z1_c,64,stride=2)\n",
        "one_blk = tf.keras.layers.Conv2D(64, (1, 1), padding='same',strides=2)(z1_c)\n",
        "z2 = tf.keras.layers.add([blk2,one_blk])\n",
        "\n",
        "blk2_c = ResNetBlock(z2,64)\n",
        "z2_c = tf.keras.layers.add([blk2_c,z2])\n",
        "\n",
        "\n",
        "\n",
        "##block3\n",
        "\n",
        "blk3 = ResNetBlock(z2_c,128,stride=2)\n",
        "one_blk_1 = tf.keras.layers.Conv2D(128, (1, 1), padding='same',strides=2)(z2_c)\n",
        "z3 = tf.keras.layers.add([blk3,one_blk_1])\n",
        "\n",
        "blk3_c = ResNetBlock(z3,128)\n",
        "z3_c = tf.keras.layers.add([blk3_c,z3])\n",
        "\n",
        "\n",
        "##block4\n",
        "\n",
        "blk4 = ResNetBlock(z3_c,256,stride=2)\n",
        "one_blk_2 = tf.keras.layers.Conv2D(256, (1, 1), padding='same',strides=2)(z3_c)\n",
        "z4 = tf.keras.layers.add([blk4,one_blk_2])\n",
        "\n",
        "blk4_c = ResNetBlock(z4,256)\n",
        "z4_c = tf.keras.layers.add([blk4_c,z4])\n",
        "\n",
        "avg_pool_layer = tf.keras.layers.GlobalAveragePooling2D()(z4_c)\n",
        "\n",
        "#flatten_layer = Flatten()(avg_pool_layer)\n",
        "\n",
        "fc_layer = tf.keras.layers.Dense(10, activation='softmax')(avg_pool_layer)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= fc_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sV-nVdlMDwva",
        "outputId": "fc65f640-2fee-43c6-8743-339f25031d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9216        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9216        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9216        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 32)   0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   18432       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   36864       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   2112        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 128)    73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    147456      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 128)    8320        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 128)    0           conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147456      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 128)    0           conv2d_14[0][0]                  \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 128)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 256)    294912      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 256)    1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    589824      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 256)    0           conv2d_16[0][0]                  \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    589824      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    589824      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 256)    0           conv2d_19[0][0]                  \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 256)          0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 2,800,170\n",
            "Trainable params: 2,796,714\n",
            "Non-trainable params: 3,456\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZl_uafGb3r9",
        "colab": {}
      },
      "source": [
        "# #from one_cycle_lr import LRFinder\n",
        "# from one_cycle_lr_tf import LRFinder\n",
        "# num_samples= train_features.shape[0]\n",
        "# batch_size =512\n",
        "# num_epoch=50\n",
        "# max_lr=0.05\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDMPrA1Gj01s"
      },
      "source": [
        "#Best LR would be 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPvfJUU1sn1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def truncate(n, decimals=0):\n",
        "#     multiplier = 10 ** decimals\n",
        "#     return int(n * multiplier) / multiplier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5l4Uuj2MLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## for 24 epochs only\n",
        "\n",
        "MAX_LR= 0.12\n",
        "base_lr = 0.01\n",
        "\n",
        "def lr_func(epoch,lr):\n",
        "  lr = base_lr\n",
        "  max_lr = MAX_LR\n",
        "  decay=0.1\n",
        "  if(epoch == 0):\n",
        "    lr = base_lr\n",
        "  elif(epoch>0 and epoch<7):\n",
        "    lr += (max_lr-base_lr)*(epoch)/8\n",
        "  else:\n",
        "    lr = max_lr - (max_lr-base_lr)*(epoch-7)/15\n",
        "  \n",
        "  print(\"final lr \",round(lr,7))\n",
        "  return round(lr,7)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjzOgXaWj_uE",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XaDmx-zQlExS",
        "colab": {}
      },
      "source": [
        "\n",
        "# from one_cycle_lr_tf import OneCycleLR\n",
        "\n",
        "# lr_manager = OneCycleLR(num_samples, num_epoch, batch_size, max_lr,\n",
        "#                         end_percentage=0.1, scale_percentage=None,\n",
        "#                         maximum_momentum=0.95, minimum_momentum=0.85)\n",
        "                        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "upsNiPO4JWjb",
        "outputId": "619bba42-96fe-4653-e392-99689fb83ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath = \"Resnet-13-test1.hdf5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model_info = model.fit_generator(train_generator,\n",
        "                                 steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "                                 validation_data = test_generator, verbose=1,callbacks=[checkpoint,LearningRateScheduler(lr_func, verbose=1)])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final lr  0.01\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.5669 - acc: 0.4349\n",
            "Epoch 00001: val_acc improved from -inf to 0.53150, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 28s 71ms/step - loss: 1.5655 - acc: 0.4355 - val_loss: 1.2895 - val_acc: 0.5315\n",
            "final lr  0.02375\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.02375.\n",
            "Epoch 2/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.2178 - acc: 0.5641\n",
            "Epoch 00002: val_acc improved from 0.53150 to 0.57060, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 1.2175 - acc: 0.5641 - val_loss: 1.2344 - val_acc: 0.5706\n",
            "final lr  0.0375\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0375.\n",
            "Epoch 3/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.0521 - acc: 0.6336\n",
            "Epoch 00003: val_acc improved from 0.57060 to 0.67370, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 1.0514 - acc: 0.6338 - val_loss: 0.9587 - val_acc: 0.6737\n",
            "final lr  0.05125\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05125.\n",
            "Epoch 4/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.9479 - acc: 0.6736\n",
            "Epoch 00004: val_acc did not improve from 0.67370\n",
            "391/391 [==============================] - 20s 50ms/step - loss: 0.9476 - acc: 0.6738 - val_loss: 1.1225 - val_acc: 0.6613\n",
            "final lr  0.065\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.065.\n",
            "Epoch 5/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8922 - acc: 0.6978\n",
            "Epoch 00005: val_acc improved from 0.67370 to 0.76260, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.8918 - acc: 0.6980 - val_loss: 0.6777 - val_acc: 0.7626\n",
            "final lr  0.07875\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.07875.\n",
            "Epoch 6/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.8003 - acc: 0.7300\n",
            "Epoch 00006: val_acc did not improve from 0.76260\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.8001 - acc: 0.7300 - val_loss: 0.8091 - val_acc: 0.7478\n",
            "final lr  0.0925\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0925.\n",
            "Epoch 7/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7319 - acc: 0.7502\n",
            "Epoch 00007: val_acc improved from 0.76260 to 0.78260, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.7314 - acc: 0.7504 - val_loss: 0.6883 - val_acc: 0.7826\n",
            "final lr  0.12\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.12.\n",
            "Epoch 8/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7216 - acc: 0.7571\n",
            "Epoch 00008: val_acc did not improve from 0.78260\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.7216 - acc: 0.7571 - val_loss: 0.7403 - val_acc: 0.7486\n",
            "final lr  0.1126667\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1126667.\n",
            "Epoch 9/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6351 - acc: 0.7833\n",
            "Epoch 00009: val_acc improved from 0.78260 to 0.81170, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.6350 - acc: 0.7833 - val_loss: 0.5563 - val_acc: 0.8117\n",
            "final lr  0.1053333\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1053333.\n",
            "Epoch 10/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8051\n",
            "Epoch 00010: val_acc did not improve from 0.81170\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5734 - acc: 0.8050 - val_loss: 0.5886 - val_acc: 0.7993\n",
            "final lr  0.098\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.098.\n",
            "Epoch 11/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5373 - acc: 0.8153\n",
            "Epoch 00011: val_acc did not improve from 0.81170\n",
            "391/391 [==============================] - 20s 51ms/step - loss: 0.5373 - acc: 0.8153 - val_loss: 0.5974 - val_acc: 0.8049\n",
            "final lr  0.0906667\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0906667.\n",
            "Epoch 12/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.5037 - acc: 0.8271\n",
            "Epoch 00012: val_acc improved from 0.81170 to 0.84180, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.5034 - acc: 0.8271 - val_loss: 0.4693 - val_acc: 0.8418\n",
            "final lr  0.0833333\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0833333.\n",
            "Epoch 13/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4591 - acc: 0.8414\n",
            "Epoch 00013: val_acc improved from 0.84180 to 0.84530, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.4595 - acc: 0.8412 - val_loss: 0.4648 - val_acc: 0.8453\n",
            "final lr  0.076\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.076.\n",
            "Epoch 14/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4255 - acc: 0.8522\n",
            "Epoch 00014: val_acc improved from 0.84530 to 0.85810, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.4254 - acc: 0.8522 - val_loss: 0.4235 - val_acc: 0.8581\n",
            "final lr  0.0686667\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0686667.\n",
            "Epoch 15/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.3971 - acc: 0.8643\n",
            "Epoch 00015: val_acc improved from 0.85810 to 0.86700, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3965 - acc: 0.8644 - val_loss: 0.3941 - val_acc: 0.8670\n",
            "final lr  0.0613333\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0613333.\n",
            "Epoch 16/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3686 - acc: 0.8723\n",
            "Epoch 00016: val_acc improved from 0.86700 to 0.87180, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3687 - acc: 0.8722 - val_loss: 0.3966 - val_acc: 0.8718\n",
            "final lr  0.054\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.054.\n",
            "Epoch 17/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3420 - acc: 0.8821\n",
            "Epoch 00017: val_acc improved from 0.87180 to 0.87920, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3421 - acc: 0.8820 - val_loss: 0.3631 - val_acc: 0.8792\n",
            "final lr  0.0466667\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0466667.\n",
            "Epoch 18/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.8892\n",
            "Epoch 00018: val_acc improved from 0.87920 to 0.88340, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.3195 - acc: 0.8890 - val_loss: 0.3462 - val_acc: 0.8834\n",
            "final lr  0.0393333\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0393333.\n",
            "Epoch 19/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9002\n",
            "Epoch 00019: val_acc improved from 0.88340 to 0.88680, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.2903 - acc: 0.9001 - val_loss: 0.3485 - val_acc: 0.8868\n",
            "final lr  0.032\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.032.\n",
            "Epoch 20/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.2771 - acc: 0.9022\n",
            "Epoch 00020: val_acc improved from 0.88680 to 0.89480, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.2766 - acc: 0.9025 - val_loss: 0.3242 - val_acc: 0.8948\n",
            "final lr  0.0246667\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0246667.\n",
            "Epoch 21/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9100\n",
            "Epoch 00021: val_acc improved from 0.89480 to 0.89490, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.2517 - acc: 0.9101 - val_loss: 0.3152 - val_acc: 0.8949\n",
            "final lr  0.0173333\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0173333.\n",
            "Epoch 22/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9183\n",
            "Epoch 00022: val_acc improved from 0.89490 to 0.89810, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.2338 - acc: 0.9182 - val_loss: 0.3045 - val_acc: 0.8981\n",
            "final lr  0.01\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 23/24\n",
            "383/391 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9244"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lMNqRYEAzTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}