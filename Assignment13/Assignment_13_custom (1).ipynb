{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment_13_custom.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sw4nNhM8mN2z",
        "outputId": "bd4545d9-459b-48cc-8e84-8b3871f0ff73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import cv2\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VeLRe4cBmbu_",
        "outputId": "20cadde4-4a8b-4dd3-a240-51c486710f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9Men1oDmesN",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q5EigjQNmhWU",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_random_crop(p=0.5,s_l=0.05,s_h=0.3,r_1=0.3,r_2=1/0.3,max_erasers_per_image=1,pixel_level=True,random_crop_size=(32,32),padding_pixels=4):\n",
        "  \n",
        "  assert max_erasers_per_image>=1 \n",
        "  def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        mx = np.random.randint(1,max_erasers_per_image+1)\n",
        "        for i in range(mx):\n",
        "          while True:\n",
        "              s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "              r = np.random.uniform(r_1, r_2)\n",
        "              w = int(np.sqrt(s / r))\n",
        "              h = int(np.sqrt(s * r))\n",
        "              left = np.random.randint(0, img_w)\n",
        "              top = np.random.randint(0, img_h)\n",
        "\n",
        "              if left + w <= img_w and top + h <= img_h:\n",
        "                  break\n",
        "\n",
        "          if pixel_level:\n",
        "              c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "          else:\n",
        "              c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "          input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "\n",
        "    \n",
        "  def random_crop(input_image):\n",
        "    assert input_image.shape[2]==3\n",
        "\n",
        "    #pad for 4 pixels\n",
        "    img = cv2.copyMakeBorder(input_image,padding_pixels,padding_pixels,padding_pixels,padding_pixels,cv2.BORDER_REPLICATE)\n",
        "    height , width =img.shape[0],img.shape[1]\n",
        "    dy,dx = random_crop_size\n",
        "    x = np.random.randint(0,width - dx + 1)\n",
        "    y = np.random.randint(0,height - dy + 1)\n",
        "    return img[y:(y+dy),x:(x+dx),:]\n",
        "\n",
        "  def preprocess_image(input_image):\n",
        "    return eraser(random_crop(input_image))\n",
        "  \n",
        "  return preprocess_image\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tOuprdCpqSB",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=0.5,featurewise_center=True, featurewise_std_normalization=True,preprocessing_function=get_cutout_eraser_and_random_crop())\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "#datagen.fit(train_features)\n",
        "train_generator = datagen.flow(train_features,train_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMi1CeDsqUyv",
        "colab": {}
      },
      "source": [
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "test_datagen.fit(test_features)\n",
        "test_generator = test_datagen.flow(test_features,test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Za7Xax0zyZc2"
      },
      "source": [
        "#Resnet Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xp2uGvCQcTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "85a91c42-d1ea-490f-cfb9-546f81a27e78"
      },
      "source": [
        "# import time, math\n",
        "# def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "#   fan = np.prod(shape[:-1])\n",
        "#   bound = 1 / math.sqrt(fan)\n",
        "#   return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "initializer = tf.keras.initializers.glorot_normal(seed=None)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0806 11:03:17.327159 140212432693120 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B41YNxNtCUwa",
        "colab": {}
      },
      "source": [
        "def ResNetBlock(input_layer, channels,stride=1):\n",
        "  \n",
        "  bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(input_layer)\n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  if(stride==2):\n",
        "    activation_layer_b1= tf.keras.layers.MaxPooling2D()(activation_layer_b1)\n",
        "  block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=initializer,use_bias=False)(activation_layer_b1)\n",
        "  \n",
        "  bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "  activation_layer_b2 = tf.keras.layers.Activation('relu')(bn_2) \n",
        "  block_layer_2 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=initializer,use_bias=False)(activation_layer_b2)\n",
        "   \n",
        "  \n",
        "  return block_layer_2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zl45v4UBTff",
        "colab": {}
      },
      "source": [
        "# from tf.keras.layers import Input, add, GlobalAveragePooling2D, Dense\n",
        "#from tf.keras.models import Model\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "x1 = tf.keras.layers.Conv2D(32, (3, 3),padding='same',kernel_initializer=initializer,use_bias=False)(inputs)   #32x32 \n",
        "activation_x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "bn1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x1)\n",
        "\n",
        "# x2 = tf.keras.layers.Conv2D(32, (3, 3),padding='same',kernel_initializer=initializer)(bn1)   #32x32 \n",
        "# activation_x2 = tf.keras.layers.Activation('relu')(x2)\n",
        "# bn2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x2)\n",
        "\n",
        "# x3 = tf.keras.layers.Conv2D(32, (3, 3),padding='same')(activation_x2)   #32x32 \n",
        "# activation_x3 = tf.keras.layers.Activation('relu')(x3)\n",
        "# bn3 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x3)\n",
        "\n",
        "##block 1\n",
        "\n",
        "blk1 = ResNetBlock(bn1,32)  ##32x32\n",
        "z1 = tf.keras.layers.add([blk1,bn1])\n",
        "\n",
        "blk1_c = ResNetBlock(z1,32)\n",
        "z1_c = tf.keras.layers.add([blk1_c,z1])\n",
        "\n",
        "\n",
        "\n",
        "##block 2\n",
        "\n",
        "blk2 = ResNetBlock(z1_c,64,stride=2)\n",
        "one_blk = tf.keras.layers.Conv2D(64, (1, 1), padding='same',strides=2)(z1_c)\n",
        "z2 = tf.keras.layers.add([blk2,one_blk])\n",
        "\n",
        "blk2_c = ResNetBlock(z2,64)\n",
        "z2_c = tf.keras.layers.add([blk2_c,z2])\n",
        "\n",
        "\n",
        "\n",
        "##block3\n",
        "\n",
        "blk3 = ResNetBlock(z2_c,128,stride=2)\n",
        "one_blk_1 = tf.keras.layers.Conv2D(128, (1, 1), padding='same',strides=2)(z2_c)\n",
        "z3 = tf.keras.layers.add([blk3,one_blk_1])\n",
        "\n",
        "blk3_c = ResNetBlock(z3,128)\n",
        "z3_c = tf.keras.layers.add([blk3_c,z3])\n",
        "\n",
        "\n",
        "##block4\n",
        "\n",
        "# blk4 = ResNetBlock(drp3,128,stride=2)\n",
        "# one_blk_2 = tf.keras.layers.Conv2D(128, (1, 1), padding='same',strides=2)(drp3)\n",
        "# z4 = tf.keras.layers.add([blk4,one_blk_2])\n",
        "\n",
        "# blk4_c = ResNetBlock(z4,128)\n",
        "# z4_c = tf.keras.layers.add([blk4_c,z4])\n",
        "\n",
        "avg_pool_layer = tf.keras.layers.GlobalAveragePooling2D()(z3_c)\n",
        "\n",
        "#flatten_layer = Flatten()(avg_pool_layer)\n",
        "\n",
        "fc_layer = tf.keras.layers.Dense(10, activation='softmax')(avg_pool_layer)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= fc_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sV-nVdlMDwva",
        "outputId": "1b384492-2ba7-42bc-b65f-1997b6a89a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9216        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   9216        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 32)   9216        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 32)   0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   18432       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   36864       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   2112        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)     0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 128)    73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    147456      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 128)    8320        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 128)    0           conv2d_11[0][0]                  \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147456      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 128)    0           conv2d_14[0][0]                  \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 128)          0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           1290        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 697,898\n",
            "Trainable params: 696,234\n",
            "Non-trainable params: 1,664\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZl_uafGb3r9",
        "colab": {}
      },
      "source": [
        "# #from one_cycle_lr import LRFinder\n",
        "# from one_cycle_lr_tf import LRFinder\n",
        "# num_samples= train_features.shape[0]\n",
        "# batch_size =512\n",
        "# num_epoch=50\n",
        "# max_lr=0.05\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDMPrA1Gj01s"
      },
      "source": [
        "#Best LR would be 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPvfJUU1sn1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def truncate(n, decimals=0):\n",
        "#     multiplier = 10 ** decimals\n",
        "#     return int(n * multiplier) / multiplier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5l4Uuj2MLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## for 24 epochs only\n",
        "\n",
        "MAX_LR= 0.2\n",
        "base_lr = 0.02\n",
        "\n",
        "def lr_func(epoch,lr):\n",
        "  lr = base_lr\n",
        "  max_lr = MAX_LR\n",
        "  \n",
        "  if(epoch == 0):\n",
        "    lr = base_lr\n",
        "  elif(epoch>0 and epoch<7):\n",
        "    lr += (max_lr-base_lr)*(epoch)/7\n",
        "  else:\n",
        "    lr = max_lr - (max_lr-base_lr)*(epoch-7)/16\n",
        "  print(\"final lr \",round(lr,5))\n",
        "  return round(lr,5)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PjzOgXaWj_uE",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XaDmx-zQlExS",
        "colab": {}
      },
      "source": [
        "\n",
        "# from one_cycle_lr_tf import OneCycleLR\n",
        "\n",
        "# lr_manager = OneCycleLR(num_samples, num_epoch, batch_size, max_lr,\n",
        "#                         end_percentage=0.1, scale_percentage=None,\n",
        "#                         maximum_momentum=0.95, minimum_momentum=0.85)\n",
        "                        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "upsNiPO4JWjb",
        "outputId": "02e3754c-5705-48f6-ca99-2301a24a7607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath = \"Resnet-13-test1.hdf5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model_info = model.fit_generator(train_generator,\n",
        "                                 steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "                                 validation_data = test_generator, verbose=1,callbacks=[checkpoint,LearningRateScheduler(lr_func, verbose=1)])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final lr  0.02\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.02.\n",
            "Epoch 1/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.5535 - acc: 0.4382\n",
            "Epoch 00001: val_acc improved from -inf to 0.56640, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 24s 62ms/step - loss: 1.5513 - acc: 0.4391 - val_loss: 1.2102 - val_acc: 0.5664\n",
            "final lr  0.04571\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.04571.\n",
            "Epoch 2/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.2288 - acc: 0.5593\n",
            "Epoch 00002: val_acc improved from 0.56640 to 0.60410, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 1.2280 - acc: 0.5597 - val_loss: 1.1317 - val_acc: 0.6041\n",
            "final lr  0.07143\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.07143.\n",
            "Epoch 3/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.6211\n",
            "Epoch 00003: val_acc improved from 0.60410 to 0.68300, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 1.0675 - acc: 0.6212 - val_loss: 0.9395 - val_acc: 0.6830\n",
            "final lr  0.09714\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.09714.\n",
            "Epoch 4/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.9241 - acc: 0.6784\n",
            "Epoch 00004: val_acc improved from 0.68300 to 0.68330, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 47ms/step - loss: 0.9236 - acc: 0.6785 - val_loss: 0.9649 - val_acc: 0.6833\n",
            "final lr  0.12286\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.12286.\n",
            "Epoch 5/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8405 - acc: 0.7081\n",
            "Epoch 00005: val_acc improved from 0.68330 to 0.70550, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.8402 - acc: 0.7082 - val_loss: 0.8438 - val_acc: 0.7055\n",
            "final lr  0.14857\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.14857.\n",
            "Epoch 6/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.7951 - acc: 0.7255\n",
            "Epoch 00006: val_acc did not improve from 0.70550\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.7943 - acc: 0.7258 - val_loss: 1.0222 - val_acc: 0.6939\n",
            "final lr  0.17429\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.17429.\n",
            "Epoch 7/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7960 - acc: 0.7268\n",
            "Epoch 00007: val_acc improved from 0.70550 to 0.71390, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.7957 - acc: 0.7269 - val_loss: 0.9378 - val_acc: 0.7139\n",
            "final lr  0.2\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.2.\n",
            "Epoch 8/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.8162 - acc: 0.7231\n",
            "Epoch 00008: val_acc did not improve from 0.71390\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.8163 - acc: 0.7232 - val_loss: 1.0950 - val_acc: 0.6708\n",
            "final lr  0.18875\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.18875.\n",
            "Epoch 9/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.7284 - acc: 0.7537\n",
            "Epoch 00009: val_acc improved from 0.71390 to 0.75380, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.7286 - acc: 0.7536 - val_loss: 0.7339 - val_acc: 0.7538\n",
            "final lr  0.1775\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1775.\n",
            "Epoch 10/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.6709 - acc: 0.7717\n",
            "Epoch 00010: val_acc improved from 0.75380 to 0.75450, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.6712 - acc: 0.7716 - val_loss: 0.7321 - val_acc: 0.7545\n",
            "final lr  0.16625\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.16625.\n",
            "Epoch 11/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.7856\n",
            "Epoch 00011: val_acc improved from 0.75450 to 0.78050, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 47ms/step - loss: 0.6285 - acc: 0.7855 - val_loss: 0.6900 - val_acc: 0.7805\n",
            "final lr  0.155\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.155.\n",
            "Epoch 12/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8046\n",
            "Epoch 00012: val_acc improved from 0.78050 to 0.80280, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.5713 - acc: 0.8047 - val_loss: 0.5982 - val_acc: 0.8028\n",
            "final lr  0.14375\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.14375.\n",
            "Epoch 13/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.5443 - acc: 0.8145\n",
            "Epoch 00013: val_acc improved from 0.80280 to 0.82470, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.5446 - acc: 0.8143 - val_loss: 0.5243 - val_acc: 0.8247\n",
            "final lr  0.1325\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1325.\n",
            "Epoch 14/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5019 - acc: 0.8268\n",
            "Epoch 00014: val_acc did not improve from 0.82470\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.5018 - acc: 0.8269 - val_loss: 0.5295 - val_acc: 0.8219\n",
            "final lr  0.12125\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.12125.\n",
            "Epoch 15/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4741 - acc: 0.8390\n",
            "Epoch 00015: val_acc improved from 0.82470 to 0.83650, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.4737 - acc: 0.8392 - val_loss: 0.5030 - val_acc: 0.8365\n",
            "final lr  0.11\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.11.\n",
            "Epoch 16/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.8462\n",
            "Epoch 00016: val_acc improved from 0.83650 to 0.86150, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.4491 - acc: 0.8462 - val_loss: 0.4171 - val_acc: 0.8615\n",
            "final lr  0.09875\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.09875.\n",
            "Epoch 17/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.4229 - acc: 0.8535\n",
            "Epoch 00017: val_acc did not improve from 0.86150\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.4227 - acc: 0.8536 - val_loss: 0.5555 - val_acc: 0.8344\n",
            "final lr  0.0875\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0875.\n",
            "Epoch 18/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8642\n",
            "Epoch 00018: val_acc improved from 0.86150 to 0.87090, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3931 - acc: 0.8642 - val_loss: 0.3871 - val_acc: 0.8709\n",
            "final lr  0.07625\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.07625.\n",
            "Epoch 19/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8713\n",
            "Epoch 00019: val_acc improved from 0.87090 to 0.87860, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3714 - acc: 0.8713 - val_loss: 0.3744 - val_acc: 0.8786\n",
            "final lr  0.065\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.065.\n",
            "Epoch 20/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8796\n",
            "Epoch 00020: val_acc improved from 0.87860 to 0.88530, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3472 - acc: 0.8796 - val_loss: 0.3452 - val_acc: 0.8853\n",
            "final lr  0.05375\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.05375.\n",
            "Epoch 21/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8871\n",
            "Epoch 00021: val_acc improved from 0.88530 to 0.88810, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3224 - acc: 0.8872 - val_loss: 0.3409 - val_acc: 0.8881\n",
            "final lr  0.0425\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0425.\n",
            "Epoch 22/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.3002 - acc: 0.8942\n",
            "Epoch 00022: val_acc improved from 0.88810 to 0.89400, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3004 - acc: 0.8943 - val_loss: 0.3281 - val_acc: 0.8940\n",
            "final lr  0.03125\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.03125.\n",
            "Epoch 23/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9023\n",
            "Epoch 00023: val_acc improved from 0.89400 to 0.89660, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.2825 - acc: 0.9024 - val_loss: 0.3115 - val_acc: 0.8966\n",
            "final lr  0.02\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.02.\n",
            "Epoch 24/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2623 - acc: 0.9072\n",
            "Epoch 00024: val_acc improved from 0.89660 to 0.90080, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.2622 - acc: 0.9072 - val_loss: 0.3111 - val_acc: 0.9008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXaHWm9EDWQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ADKRdQnkKQ_z",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGlxPxyywoIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lMNqRYEAzTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}